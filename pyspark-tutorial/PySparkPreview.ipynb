{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion Week 3: PySpark Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Comprehensions, Lambdas, Generators, and Yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16]\n",
      "[0, 1, 0, 81, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 81, 0, 0, 4, 0, 324, 0, 0, 9, 0, 729, 0, 0, 16, 0, 1296, 0]\n"
     ]
    }
   ],
   "source": [
    "# List comprehensions:\n",
    "x = range(100)\n",
    "\n",
    "y = [n**2 for n in x if n < 5]\n",
    "\n",
    "print y\n",
    "\n",
    "y2 = [n**2 if n % 2 else 0 for n in y]\n",
    "print y2\n",
    "\n",
    "print [a * b for a in y for b in y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lambda Expressions\n",
    "\n",
    "def convert_me(n):\n",
    "    return 1./ n ** 2\n",
    "\n",
    "convert_you = lambda x: 1./x ** 2\n",
    "\n",
    "convert_me(10) == convert_you(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen1 = lambda n: (i for i in range(n))\n",
    "\n",
    "def gen2(n):\n",
    "    for i in range(n):\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "g1 = gen1(5)\n",
    "g2 = gen2(5)\n",
    "for i in range(5):\n",
    "    print next(g1) == next(g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'cs186_spark', 'python', 'lib', 'pyspark.zip'))\n",
    "sys.path.append(os.path.join(os.getcwd(), 'cs186_spark', 'python', 'lib', 'py4j-0.9-src.zip'))\n",
    "os.environ[\"SPARK_HOME\"] = os.path.join(os.getcwd(), 'cs186_spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "n = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load new RDD using a collection from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = n.parallelize(range(10), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rdd.collect()` return a list that contains all of the elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of making our own collection, let's load in a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = n.textFile('link_text.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the first few entries in this document - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the Raw document\n",
      "http://gitxiv.com/posts/tfkjEgw9x4KSi2GnH/long-short-term-memory-networks-for-machine-reading\r\n",
      "http://fivethirtyeight.com/features/the-rent-seeking-is-too-damn-high/\r\n",
      "http://nautil.us/issue/33/attraction/love-is-like-cocaine\r\n",
      "http://swiftmonthly.com/issues/latest/?feb2016\r\n",
      "http://arstechnica.com/tech-policy/2016/02/profs-protest-invasive-cybersecurity-measures-at-university-of-california-campuses/\r\n"
     ]
    }
   ],
   "source": [
    "print \"Here is the Raw document\"\n",
    "\n",
    "!head -n 5 link_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'http://gitxiv.com/posts/tfkjEgw9x4KSi2GnH/long-short-term-memory-networks-for-machine-reading',\n",
       " u'http://fivethirtyeight.com/features/the-rent-seeking-is-too-damn-high/',\n",
       " u'http://nautil.us/issue/33/attraction/love-is-like-cocaine',\n",
       " u'http://swiftmonthly.com/issues/latest/?feb2016',\n",
       " u'http://arstechnica.com/tech-policy/2016/02/profs-protest-invasive-cybersecurity-measures-at-university-of-california-campuses/']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something interesting with this data - get the domains of all of the websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_site(iterator):\n",
    "    for link in iterator:\n",
    "        index = link.find(\"www.\")\n",
    "        end = link.find(\".com\")\n",
    "        if index > 0 and end > 0:\n",
    "            yield link[index + 4: end]\n",
    "\n",
    "site_rdd = rdd2.mapPartitionsWithIndex(lambda index, iterator: get_site(iterator))\n",
    "# notice how i toss out the index\n",
    "# also notice how nothing happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'nytimes', u'bloomberg', u'righto', u'clicktorelease', u'nytimes']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 685\n"
     ]
    }
   ],
   "source": [
    "print rdd2.getNumPartitions(), rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice how the object itself is not very eventful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the raw implementation of `rdd.distinct` from PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distinct(self, numPartitions=None):\n",
    "    \"\"\"\n",
    "    Return a new RDD containing the distinct elements in this RDD.\n",
    "\n",
    "    >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
    "    [1, 2, 3]\n",
    "    \"\"\"\n",
    "    return self.map(lambda x: (x, None)) \\\n",
    "               .reduceByKey(lambda x, _: x, numPartitions) \\\n",
    "               .map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get the distinct URLs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'',\n",
       " u'cnbc',\n",
       " u'bloomberg',\n",
       " u'gq',\n",
       " u'timesofisrael',\n",
       " u'thestar',\n",
       " u'reddit',\n",
       " u'linkedin',\n",
       " u'easypost',\n",
       " u'rbmojournal']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_rdd.distinct().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest is left as an optional exercise - can you guess what's going on below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (7, 1), (2, 2), (11, 3), (8, 4)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redrdd = n.parallelize(range(24), 4)\n",
    "testrdd = redrdd.map(lambda x: ((x ** 2 * 7) % 13 , x ))\n",
    "testrdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkit(rdd):\n",
    "    def p(n, itr):\n",
    "        for i in itr:\n",
    "            yield (n, i)\n",
    "    return rdd.mapPartitionsWithIndex(p).collect()\n",
    "\n",
    "testrdd = testrdd.reduceByKey(lambda x, y: x + y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 13), (6, 52), (7, 27), (8, 52), (2, 28)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testrdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (0, 13)),\n",
       " (0, (6, 52)),\n",
       " (1, (7, 27)),\n",
       " (2, (8, 52)),\n",
       " (2, (2, 28)),\n",
       " (2, (11, 52)),\n",
       " (2, (5, 52))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkit(testrdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
